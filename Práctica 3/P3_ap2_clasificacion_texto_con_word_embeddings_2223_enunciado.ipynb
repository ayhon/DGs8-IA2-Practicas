{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUBRwgEu3yu1"
      },
      "source": [
        "# Práctica 2: Procesamiento del Lenguaje Natural\n",
        "\n",
        "__Fecha de entrega: 8 de mayo de 2023__\n",
        "\n",
        "> El objetivo de esta práctica es aplicar los conceptos teóricos vistos en clase en el módulo de PLN. La práctica consta de 2 notebooks que se entregarán simultáneamente en la tarea de entrega habilitada en el Campus  Virtual.\n",
        ">\n",
        ">Lo más importante en esta práctica no es el código Python, sino el análisis de los datos y modelos que construyas y las explicaciones razonadas de cada una de las decisiones que tomes. __No se valorarán trozos de código o gráficas sin ningún tipo de contexto o explicación__.\n",
        ">\n",
        "> Finalmente, recuerda establecer el parámetro `random_state` en todas las funciones que tomen decisiones aleatorias para que los resultados sean reproducibles (los resultados no varíen entre ejecuciones)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3YxCTUW3yu9"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 1234"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn_YQLVL3yvA"
      },
      "source": [
        "# Apartado 1: Análisis de sentimientos con word embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de-i8w0s3yvC"
      },
      "source": [
        "__Número de grupo: 1__\n",
        "\n",
        "__Nombres de los estudiantes: Fernando Isaías Leal Sánchez y Jinqing Cai__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeVD_g2D3yvC"
      },
      "source": [
        "## 1) Carga del conjunto de datos\n",
        "\n",
        "> El fichero `IMBD_Dataset.csv` contiene opiniones de películas clasificadas en 2 categorías diferentes (positiva/negativa).\n",
        ">\n",
        "> Este set de datos se creó utilizando el \"IMDB Dataset of 50K Movie Reviews\", el cual contiene 50,000 reseñas de películas con un sentimiento positivo o negativo adjunto a ellas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YyPy4BDfzGQ",
        "outputId": "65940796-43bb-4bbb-d354-19926047dff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# acceso a google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0csu2B8N3yvE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cambiamos la representación del `sentiment`: 1 si es positivo, y 0 si es negativo. La razón es porque la librería `keras` exige que la segunda columna sea de tipo numérico, y dará excepción si es de tipo `String`."
      ],
      "metadata": {
        "id": "R_tS6IoKhV3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kmcEfPgYf1Fk",
        "outputId": "dfbc7c2f-3de1-4065-9f01-4514aa421553"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production. <br /><br />The...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-645f63cd-16fd-4057-9c0c-d272a81d3d74\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-645f63cd-16fd-4057-9c0c-d272a81d3d74')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-645f63cd-16fd-4057-9c0c-d272a81d3d74 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-645f63cd-16fd-4057-9c0c-d272a81d3d74');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "imbd_file = '/content/drive/MyDrive/IA2/IMDB_Dataset.csv'\n",
        "\n",
        "df = pd.read_csv(imbd_file)\n",
        "df['sentiment'] = np.array(df['sentiment'] == 'positive', dtype=int)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Cn6SLJX0AE"
      },
      "source": [
        "> Muestra un ejemplo de cada clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "fxgAnYdTYDds",
        "outputId": "4ac52a78-5172-46b2-cff4-6d749f189feb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "0  One of the other reviewers has mentioned that ...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0806185-6a6f-4183-9e96-7cd3c7ccbc8a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0806185-6a6f-4183-9e96-7cd3c7ccbc8a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0806185-6a6f-4183-9e96-7cd3c7ccbc8a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0806185-6a6f-4183-9e96-7cd3c7ccbc8a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Hay dos clases: positivo o negativo\n",
        "pd.concat(df[df.sentiment == sentiment].head(1) for sentiment in (0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cLEZqgfaG7v"
      },
      "source": [
        "> Haz un estudio del conjunto de datos. ¿qué palabras aparecen más veces?, ¿tendría sentido normalizar de alguna manera el corpus?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiv7YCgXaG7v"
      },
      "source": [
        "Lo primero, para contar el número de apariciones de una palabra en el conjunto de datos, pasamos a transformar nuestros documentos en bolsas de palabras. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0yObw5naG7w",
        "outputId": "0fc4f016-064e-4c24-c5c7-3e5bfacc60c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<50000x101583 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 4434500 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "# Tomamos los textos del conjunto de entrenamiento y los transformamos en \n",
        "# una matriz de datos (palabras) según el diccionario estándar\n",
        "doc_word_freq = vectorizer.fit_transform(df.review)\n",
        "doc_word_freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9-Ybj9yaG7w"
      },
      "source": [
        "`vector_data` es ahora una matriz de $50,000 \\times 101,583$ donde para cada review $r$ y cada palabra $i$ nos asocia el número de apariciones de $i$ en $r$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDrG2x1EaG7w",
        "outputId": "761b013f-3e09-49ac-af39-9eeda3d25e6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('12', 2)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "#print([(i,count) for i in range(vector_data.shape[0]) if (count := vector_data[11,i]) > 0])\n",
        "r = 11\n",
        "i = 151\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "feature_names[i], doc_word_freq[r,i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fReY_GU5aG7w"
      },
      "source": [
        "Esto nos dice que hay 2 apariciones de la palabra 151 (que se corresponde con el string `\"12\"`) en la review 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvw-YrIPaG7x",
        "outputId": "9499f97f-0680-4d03-b838-2ece4d6c91f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "review_11 = df.loc[11].review\n",
        "palabra_151 = feature_names[151]\n",
        "review_11.count(palabra_151) == doc_word_freq[r,i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmQ1sLsVaG7x"
      },
      "source": [
        "Para sacar la palabra que aparece con más frecuencia, hacemos una **suma de las apariciones de cada palabra** en general, y buscamos aquella "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XNEgBziaG7x"
      },
      "outputs": [],
      "source": [
        "def get_top_10_freq_words(vector_data):\n",
        "    word_freq = np.squeeze(np.asarray(vector_data.sum(axis=0)))\n",
        "    top_10_freq_words = word_freq.argsort()[::-1][:10]\n",
        "    return top_10_freq_words, word_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbaufEShaG7x",
        "outputId": "81c234f8-24fa-4ed1-a40c-79fc4c32f0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position 1: br with frequency 201951\n",
            "Position 2: movie with frequency 87971\n",
            "Position 3: film with frequency 79705\n",
            "Position 4: like with frequency 40172\n",
            "Position 5: just with frequency 35184\n",
            "Position 6: good with frequency 29753\n",
            "Position 7: time with frequency 25110\n",
            "Position 8: story with frequency 23119\n",
            "Position 9: really with frequency 23094\n",
            "Position 10: bad with frequency 18473\n"
          ]
        }
      ],
      "source": [
        "top_10_freq_words, word_freq = get_top_10_freq_words(doc_word_freq)\n",
        "for top, word in enumerate(top_10_freq_words):\n",
        "    print(f\"Position {top + 1}: {feature_names[word]} with frequency {word_freq[word]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NdGjY7LaG7y"
      },
      "source": [
        "Aparece palabras como `br`, que es la etiqueta de HTML para hacer cambio de línea (normal que aparezca en nuestro dataset, pues es conjunto de reseñas de películas extraidas de una página web).\n",
        "\n",
        "Por lo tanto, es necesario que hacer un preprocesamiento del dataset eliminando a estas palabras que no aportan ninguna información a la hora de clasificar los textos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCxQ5QP1aG7y"
      },
      "source": [
        "Vamos a probar **normalizar** los documentos, y ver cómo cambia:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEoP7drGaG7y"
      },
      "source": [
        "Para normalizar los documentos, quitamos todos los caracteres que no sean letras del abecedario o espacios, pasamos todo el texto a minúsculas y le quitamos los espacios de inicio y final. Una vez hecho esto, usamos el `WordPunctTokenizer` de `nltk` para separar los elementos por palabras, y hacemos un último filtro en el que quitamos palabras de la lista `stop_words`. Esta lista la hemos obtenido de `nltk` también, pero además hemos añadido `br`, puesto que no ofrece ninguna información semántica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7QPz58ZaG7y",
        "outputId": "a10c74dd-ee88-4352-a3b3-80569962061d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english') + ['br']\n",
        "\n",
        "@np.vectorize\n",
        "def normalize_corpus(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-eHA81RaG7y"
      },
      "source": [
        "Procedemos a aplicar la normalización descrita a las reseñas de nuestros datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifd_41UWaG7y"
      },
      "outputs": [],
      "source": [
        "normalized_reviews = normalize_corpus(df.review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRGl-rXgaG7y"
      },
      "source": [
        "Y repetimos el estudio de las palabras más frecuentes que hicimos antes, pero con los documentos normalizados "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZREJBBjaG7z",
        "outputId": "5f454d44-a119-4989-c526-2c96092441c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position 1: movie with frequency 83536\n",
            "Position 2: film with frequency 74478\n",
            "Position 3: like with frequency 39001\n",
            "Position 4: good with frequency 28575\n",
            "Position 5: time with frequency 23276\n",
            "Position 6: really with frequency 22951\n",
            "Position 7: story with frequency 22103\n",
            "Position 8: great with frequency 17821\n",
            "Position 9: bad with frequency 17720\n",
            "Position 10: people with frequency 17542\n"
          ]
        }
      ],
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "doc_word_freq = vectorizer.fit_transform(normalized_reviews)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "word_freq = np.squeeze(np.asarray(doc_word_freq.sum(axis=0)))\n",
        "\n",
        "top_10_freq_words = word_freq.argsort()[::-1][:10]\n",
        "for top, word in enumerate(top_10_freq_words):\n",
        "    print(f\"Position {top + 1}: {feature_names[word]} with frequency {word_freq[word]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWPBdR4XaG7z"
      },
      "source": [
        "Podemos observar que ahora las palabras más frecuentes son palabras que nos dan más información. Tiene sentido que `movie` y `film` sean las palabras más usadas, puesto que nuestro banco de datos son de reseñas de películas. También tiene sentido que aparezcan palabras como `like`, `good`, `great` o `bad`, pues la gente ha escrito las reseñas con la intención de compartir su opinión de las películas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fkxLL4ZaG7z"
      },
      "source": [
        "**De ahora en adelante, usaremos el corpus normalizado**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nCkaocgVaG7z",
        "outputId": "a56954cc-ccfa-4efa-d750-75301d4f3933"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             reviews  sentiment\n",
              "0  one reviewers mentioned watching oz episode yo...          1\n",
              "1  wonderful little production filming technique ...          1\n",
              "2  thought wonderful way spend time hot summer we...          1\n",
              "3  basically theres family little boy jake thinks...          0\n",
              "4  petter matteis love time money visually stunni...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-218fde44-494b-47e6-b3dd-45f2171e652f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one reviewers mentioned watching oz episode yo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wonderful little production filming technique ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically theres family little boy jake thinks...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter matteis love time money visually stunni...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-218fde44-494b-47e6-b3dd-45f2171e652f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-218fde44-494b-47e6-b3dd-45f2171e652f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-218fde44-494b-47e6-b3dd-45f2171e652f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "df2 = pd.DataFrame({\"reviews\": normalized_reviews, \"sentiment\": df.sentiment})\n",
        "df_sin_normalizar = df\n",
        "df = df2\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Oy0ArwaG7z"
      },
      "source": [
        "> Crea una partición de los datos dejando el 80% para entrenamiento y el 20% restante para test usando la función `train_test_split` de sklearn. Comprueba que la distribución de los ejemplos en las clases es la misma en entrenamiento y test. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyhw4yqSl211"
      },
      "source": [
        "Antes de separar los datos en dos conjuntos, vamos a pasar los reviews por un Tokenizer, y convertirlos en `Sequence`. De esta forma se trabaja mejor con Word Embedding.\n",
        "\n",
        "El `Tokenizer` devuelve una secuencia de palabras, que se corresponde con una lista de enteros, índices de las palabras a un diccionario. De la documentación de Keras:\n",
        "> This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
        "\n",
        "Podemos recuperar el texto dada una sequencia accediendo al diccinario interno del `tokenizer`, llamado `index_word` (Pues vincula índices con palabras, al contrario que `word_inex`, que vincula palabras con índices)\n",
        "\n",
        "Hemos decidido que nuestras reseñas se quedaran únicamente con `20` comentarios\n",
        "como máximo. Para que todos los documentos tengan el mismo tamaño, añadimos `0`s a todo documento que se quede con menos elementos usando `pad_sequences`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oXJzukbmd-t",
        "outputId": "4c1ba8a9-11cc-4a30-c0ed-477a8ce28195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 175616 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "max_words = 1500\n",
        "max_comment_length = 20\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df.reviews)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df.reviews)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "max_words = len(word_index)\n",
        "\n",
        "# pad_sequences is used to ensure that all sequences in a list have the same length.\n",
        "data = pad_sequences(sequences, maxlen=max_comment_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí vemos la representación de la primera reseña, con cada palabra siendo representada por un vector de índices al diccionario interno de `tokenizer`.\n"
      ],
      "metadata": {
        "id": "Ii5mn4m_xbyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN_zIWbOwumV",
        "outputId": "a32a8434-a132-4a9a-9958-6b8c61d1605f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 378,  505,   14,  141,   13,  632,  696,  540, 1077,  548,  434,\n",
              "        808, 1077,  441,   56,  100,  301,   14, 1080,  387], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para comprobar que de verdad está pasando lo que hemos explicado, recuperamos las palabras asociadas a cada índice"
      ],
      "metadata": {
        "id": "Ayjxwww4wNYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_1 = np.array([tokenizer.index_word.get(x) for x in data[0]])\n",
        "review_1\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tIetfrTs4hS",
        "outputId": "0f76b37c-4ecf-4b80-a6f5-3547e1c138b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['kill', 'order', 'get', 'away', 'well', 'middle', 'class',\n",
              "       'turned', 'prison', 'due', 'lack', 'street', 'prison',\n",
              "       'experience', 'watching', 'may', 'become', 'get', 'touch', 'side'],\n",
              "      dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos comprobar que, en efecto, se trata de la primera reseña de nuestro set de datos"
      ],
      "metadata": {
        "id": "v0Shw7_ctGJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0,'reviews']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "yeyI7r_mxR7Y",
        "outputId": "cc07e5c0-cf7f-47dc-d6a3-83be644cf259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one reviewers mentioned watching oz episode youll hooked right exactly happened mebr first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordbr called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora dividimos en 80% de entrenamiento y 20% de prueba."
      ],
      "metadata": {
        "id": "7pYfmwc_i_hj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Gokcd6aG7z"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, df.sentiment, test_size=0.2, train_size=0.8, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOuei5Q4aG7z",
        "outputId": "acdbfd53-4fb2-4062-a2d9-e22fbf6d7bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de casos en train set: 40000\n",
            "Casos positivos:  19988\n",
            "Casos negativos:  20012\n",
            "Número de casos en test set: 10000\n",
            "Casos positivos:  5012\n",
            "Casos negativos:  4988\n"
          ]
        }
      ],
      "source": [
        "# Veamos que tienen la misma distribución\n",
        "print(f\"Número de casos en train set: {len(x_train)}\")\n",
        "print(\"Casos positivos: \", np.sum(y_train.values))\n",
        "print(\"Casos negativos: \", len(y_train) - np.sum(y_train.values))\n",
        "\n",
        "print(f\"Número de casos en test set: {len(x_test)}\")\n",
        "print(\"Casos positivos: \", np.sum(y_test.values))\n",
        "print(\"Casos negativos: \", len(y_test) - np.sum(y_test.values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ia2Si2uaG70"
      },
      "source": [
        "Se puede observar que la distribución de casos positivos y negativos es más o menos igual tanto en train set como en test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rXv3xX3yvG"
      },
      "source": [
        "## 2) Estudio del efecto de distintas configuraciones de word embeddings para resolver la tarea\n",
        "\n",
        "> Usa distintas configuraciones de word embeddings y discute los resultados obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlyrtYH0Y9d1"
      },
      "source": [
        "Vamos a comprar el resultado obtenido entre:\n",
        "\n",
        "- Entrenar desde cero los `Embeddings` con nuestro corpus\n",
        "- Usar embeddings pre-entrenados, congelando el modelo (sin que aprenda de nuestro corpus)\n",
        "- Usar embeddings pre-entrenados, sin congelar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJoQiJFe7zd"
      },
      "source": [
        "### Modelo 1: Entrenar desde cero con `Embedding` Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero probamos a crear nuestra propia red neuronal que intenta adivinar los embeddings de nuestro conjunto de entrenamiento. Para ello empleamos las interfaces de `tf.keras`, que usa TensorFlow para construir fácilmente redes neuronales."
      ],
      "metadata": {
        "id": "wRhABL5hnT7r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJoj-msRgDcl"
      },
      "outputs": [],
      "source": [
        "# Fijamos el tamaño de los embedding a 50 dimensiones\n",
        "embedding_dim = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos de `keras.models` el modelo de nuestra red neuronal. Queremos una red neuronal simple, cuyo único propósito sea pasar nuestros documentos (representados como una lista de índices, representando palabras) a la capa `Embedding`, que genera un vector por cada palabra de dimensión `embedding_dim`.\n",
        "\n",
        "Estos vectores los aplanamos y se los pasamos a una capa densa con la función de activación del sigmoide $\\sigma(x) =\\frac{1}{1 + e^{-x}}$ que intenta predecir el sentimiento de nuestros documentos. Un `1` implica sentimiento positivo, un `0` sentimiento negativo"
      ],
      "metadata": {
        "id": "YQRE2dPYqtJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential # Modelo de la red neuronal\n",
        "from keras.layers import Flatten, Dense, Embedding # Distintas capas que puede tener nuestra red neuronal"
      ],
      "metadata": {
        "id": "hAyfrE0Oqjuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW3DR5v2gEGd",
        "outputId": "1b10d4dd-01b7-487e-9450-825b2add8998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 50)            8780800   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 1001      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,781,801\n",
            "Trainable params: 8,781,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 0.4845 - accuracy: 0.7692 - val_loss: 0.4380 - val_accuracy: 0.7939\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3937 - accuracy: 0.8211 - val_loss: 0.4421 - val_accuracy: 0.7926\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3425 - accuracy: 0.8505 - val_loss: 0.4655 - val_accuracy: 0.7861\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2858 - accuracy: 0.8825 - val_loss: 0.5066 - val_accuracy: 0.7746\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2371 - accuracy: 0.9081 - val_loss: 0.5612 - val_accuracy: 0.7634\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.1962 - accuracy: 0.9316 - val_loss: 0.6272 - val_accuracy: 0.7548\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1629 - accuracy: 0.9460 - val_loss: 0.7013 - val_accuracy: 0.7498\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.1342 - accuracy: 0.9590 - val_loss: 0.7842 - val_accuracy: 0.7458\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1090 - accuracy: 0.9719 - val_loss: 0.8749 - val_accuracy: 0.7441\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0886 - accuracy: 0.9794 - val_loss: 0.9716 - val_accuracy: 0.7376\n",
            "Epoch 11/20\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0701 - accuracy: 0.9872 - val_loss: 1.0770 - val_accuracy: 0.7322\n",
            "Epoch 12/20\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0556 - accuracy: 0.9916 - val_loss: 1.1819 - val_accuracy: 0.7308\n",
            "Epoch 13/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0425 - accuracy: 0.9952 - val_loss: 1.2984 - val_accuracy: 0.7308\n",
            "Epoch 14/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0326 - accuracy: 0.9970 - val_loss: 1.4141 - val_accuracy: 0.7286\n",
            "Epoch 15/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0252 - accuracy: 0.9984 - val_loss: 1.5298 - val_accuracy: 0.7268\n",
            "Epoch 16/20\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0192 - accuracy: 0.9992 - val_loss: 1.6526 - val_accuracy: 0.7262\n",
            "Epoch 17/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0143 - accuracy: 0.9996 - val_loss: 1.7660 - val_accuracy: 0.7269\n",
            "Epoch 18/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0110 - accuracy: 0.9997 - val_loss: 1.8977 - val_accuracy: 0.7241\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0082 - accuracy: 0.9999 - val_loss: 2.0166 - val_accuracy: 0.7255\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0063 - accuracy: 0.9999 - val_loss: 2.1340 - val_accuracy: 0.7238\n"
          ]
        }
      ],
      "source": [
        "model1 = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_comment_length),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid') # Esta capa es la que realiza la clasificación en realidad\n",
        "])\n",
        "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model1.summary()\n",
        "history = model1.fit(x_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score1 = model1.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (score1[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt7B5763z7UQ",
        "outputId": "9fea3863-875f-4191-e38f-0d1e53a47d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 2.1340 - accuracy: 0.7238\n",
            "Accuracy: 72.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos observar que embedding ha aprendido accediendo a la salida de las capas intermedias:"
      ],
      "metadata": {
        "id": "32zzY0bn03ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "intermediate_layer_model = keras.models.Model(inputs=model1.input,\n",
        "                                 outputs=model1.layers[0].output)\n",
        "embedding_review_1 = intermediate_layer_model.predict(data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-JYjW1v0-SB",
        "outputId": "aa211a45-9e2a-48ac-c28d-fda51873835e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora tenemos en `embedding_review_1` el embedding que ha sacado nuestro modelo para las 20 palabras de la primera reseña. Observemos, por curiosidad, el embeddin de la primera palabra"
      ],
      "metadata": {
        "id": "nXh2E99Y2dQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_1[0], embedding_review_1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdwvW4ub3Wgv",
        "outputId": "a6c10628-b877-4d93-df0a-d5726dee2bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('kill',\n",
              " array([ 0.49240118, -0.06158967, -0.08537921,  0.01198269,  0.29855233,\n",
              "        -0.00182005, -0.14153896,  0.11806239, -0.36324027, -0.1639899 ,\n",
              "         0.12262842,  0.31116223,  0.26826862,  0.10656832,  0.18166742,\n",
              "         0.12048254, -0.21079075, -0.17963287, -0.00678047, -0.18901734,\n",
              "         0.22054464, -0.12437028, -0.11108056,  0.22728847, -0.14963979,\n",
              "        -0.00364058, -0.256805  , -0.14200062,  0.1679555 , -0.5051072 ,\n",
              "        -0.1786734 , -0.18728876,  0.41131008,  0.07223381, -0.03151915,\n",
              "         0.07832994,  0.09876098,  0.33823475, -0.121471  , -0.10464939,\n",
              "        -0.1332437 , -0.2762028 , -0.4143862 ,  0.10521372,  0.19203554,\n",
              "         0.06034961,  0.34253028, -0.08648186, -0.13975914, -0.18590447],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De alguna manera parece que nuestra red neuronal ha decidido representar la palabra `kill` con este vector de 50 (`embedding_dim`) valores"
      ],
      "metadata": {
        "id": "eqZ-DpuI3fNg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lycZDwvVfTON"
      },
      "source": [
        "### Modelo 2: Usar Word Embeddings ya hechos, sin reentrenar"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A veces conviene usar Word Embeddings ya entrenados previamente porque tenemos pocos datos para entrenar un modelo bueno (en nuestro caso tenemos 50K datos y no es un problema grande).\n",
        "\n",
        "Vamos a usar el Word embedding con 50 dimensiones, creado por Stanford y entrenado con 2014 English Wikipedia + Gigaword 5, basado en algoritmo de GloVe."
      ],
      "metadata": {
        "id": "rgC3upa4nxJ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_GZYeP-po7u",
        "outputId": "7c2c175b-08f6-427b-807e-bb872cce6f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 111252 word vectors.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "glove_dir = '/content/drive/MyDrive/IA2/'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos la matriz de embedding para insertar en el modelo."
      ],
      "metadata": {
        "id": "OLnJT-DBoSa6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMSHz60Tpym1"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un nuevo modelo, pero esta vez con una capa `Embedding` cuyos pesos sobreescribimos con la matriz anterior. Además, ponemos `trainable` a `False` para asegurarnos que al entrenar nuestra red neuronal, los pesos de la capa `Embedding` no cambien"
      ],
      "metadata": {
        "id": "wQ5XFjPToXzS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICXBnR0OpxR2",
        "outputId": "109f1eab-ec90-4d04-e639-6af4c399ffb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 20, 50)            8780800   \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 1001      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,781,801\n",
            "Trainable params: 1,001\n",
            "Non-trainable params: 8,780,800\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# MODELO 2. EMBEDDINGS PRE-ENTRENADOS CONGELADOS\n",
        "model2 = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_comment_length),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "model2.layers[0].set_weights([embedding_matrix])\n",
        "model2.layers[0].trainable = False\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6QMRp1Ip2He",
        "outputId": "b1fc4eb6-cd6b-4c79-99ba-d1cf391944b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.6064 - accuracy: 0.6658 - val_loss: 0.5895 - val_accuracy: 0.6884\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.5691 - accuracy: 0.7031 - val_loss: 0.5872 - val_accuracy: 0.6921\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 0.5656 - accuracy: 0.7065 - val_loss: 0.5880 - val_accuracy: 0.6883\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 0.5640 - accuracy: 0.7074 - val_loss: 0.5812 - val_accuracy: 0.6949\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 0.5637 - accuracy: 0.7081 - val_loss: 0.5808 - val_accuracy: 0.6956\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5637 - accuracy: 0.7086 - val_loss: 0.5824 - val_accuracy: 0.6913\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5630 - accuracy: 0.7083 - val_loss: 0.5892 - val_accuracy: 0.6909\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5629 - accuracy: 0.7103 - val_loss: 0.5895 - val_accuracy: 0.6909\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5634 - accuracy: 0.7095 - val_loss: 0.5859 - val_accuracy: 0.6912\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5620 - accuracy: 0.7096 - val_loss: 0.5839 - val_accuracy: 0.6923\n",
            "Epoch 11/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5622 - accuracy: 0.7089 - val_loss: 0.5834 - val_accuracy: 0.6953\n",
            "Epoch 12/20\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 0.5627 - accuracy: 0.7114 - val_loss: 0.5888 - val_accuracy: 0.6935\n",
            "Epoch 13/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5618 - accuracy: 0.7111 - val_loss: 0.5846 - val_accuracy: 0.6941\n",
            "Epoch 14/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5627 - accuracy: 0.7094 - val_loss: 0.5821 - val_accuracy: 0.6933\n",
            "Epoch 15/20\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.5625 - accuracy: 0.7088 - val_loss: 0.5811 - val_accuracy: 0.6944\n",
            "Epoch 16/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5629 - accuracy: 0.7094 - val_loss: 0.5835 - val_accuracy: 0.6923\n",
            "Epoch 17/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5621 - accuracy: 0.7118 - val_loss: 0.5828 - val_accuracy: 0.6939\n",
            "Epoch 18/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5614 - accuracy: 0.7116 - val_loss: 0.5880 - val_accuracy: 0.6888\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5621 - accuracy: 0.7082 - val_loss: 0.5812 - val_accuracy: 0.6953\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.5613 - accuracy: 0.7119 - val_loss: 0.5826 - val_accuracy: 0.6973\n"
          ]
        }
      ],
      "source": [
        "model2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model2.fit(x_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score2 = model2.evaluate(x_test, y_test)\n",
        "print(\"Accuracy: %.2f%%\" % (score2[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm2qn6LB5Dq6",
        "outputId": "8f255573-d0c9-4fc3-8246-52881324413e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5826 - accuracy: 0.6973\n",
            "Accuracy: 69.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM3KmVnRfXfl"
      },
      "source": [
        "### Modelo 3: Usar Word Embeddings ya hechos, reentrenando"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalemente, probamos a usar la matriz de GloVe que usamos anteriormente, pero sí que permitimos a nuestra red neuronal que cambie los pesos asociados a la capa de `Embedding`. De esta manera, podemos aprender un embedding más especializado, pero partiendo de la base de la matriz anterior."
      ],
      "metadata": {
        "id": "BDhPjb43or12"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AUNSmKKp3rd",
        "outputId": "cd840e33-a0f7-4b42-d448-efd6f92e2622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 20, 50)            8780800   \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 1001      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,781,801\n",
            "Trainable params: 8,781,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# MODELO3. EMBEDDINGS PREENTRENADOS SIN CONGELAR\n",
        "model3 = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_comment_length),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "model3.layers[0].set_weights([embedding_matrix])\n",
        "model3.layers[0].trainable = True\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model3.fit(x_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5o1vPHP49HM",
        "outputId": "e03df5f9-f821-4335-faf7-0a3751378ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 0.5416 - accuracy: 0.7188 - val_loss: 0.4754 - val_accuracy: 0.7721\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4251 - accuracy: 0.8008 - val_loss: 0.4476 - val_accuracy: 0.7904\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3934 - accuracy: 0.8207 - val_loss: 0.4496 - val_accuracy: 0.7930\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3690 - accuracy: 0.8326 - val_loss: 0.4666 - val_accuracy: 0.7839\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3435 - accuracy: 0.8492 - val_loss: 0.4743 - val_accuracy: 0.7850\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3166 - accuracy: 0.8642 - val_loss: 0.4987 - val_accuracy: 0.7767\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2912 - accuracy: 0.8787 - val_loss: 0.5314 - val_accuracy: 0.7702\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2662 - accuracy: 0.8906 - val_loss: 0.5559 - val_accuracy: 0.7686\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2428 - accuracy: 0.9041 - val_loss: 0.5905 - val_accuracy: 0.7618\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2207 - accuracy: 0.9162 - val_loss: 0.6243 - val_accuracy: 0.7593\n",
            "Epoch 11/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2000 - accuracy: 0.9247 - val_loss: 0.6665 - val_accuracy: 0.7575\n",
            "Epoch 12/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.1807 - accuracy: 0.9352 - val_loss: 0.7126 - val_accuracy: 0.7557\n",
            "Epoch 13/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1620 - accuracy: 0.9446 - val_loss: 0.7636 - val_accuracy: 0.7515\n",
            "Epoch 14/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.1445 - accuracy: 0.9533 - val_loss: 0.8209 - val_accuracy: 0.7457\n",
            "Epoch 15/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1277 - accuracy: 0.9614 - val_loss: 0.8804 - val_accuracy: 0.7447\n",
            "Epoch 16/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.1117 - accuracy: 0.9694 - val_loss: 0.9424 - val_accuracy: 0.7434\n",
            "Epoch 17/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0972 - accuracy: 0.9755 - val_loss: 1.0097 - val_accuracy: 0.7415\n",
            "Epoch 18/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0835 - accuracy: 0.9822 - val_loss: 1.0851 - val_accuracy: 0.7378\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0714 - accuracy: 0.9859 - val_loss: 1.1566 - val_accuracy: 0.7384\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0599 - accuracy: 0.9907 - val_loss: 1.2412 - val_accuracy: 0.7325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score3 = model3.evaluate(x_test, y_test)\n",
        "print(\"Accuracy: %.2f%%\" % (score3[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NpqBTYM5QrX",
        "outputId": "c2655f81-59c7-4afa-add8-c70332774515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2412 - accuracy: 0.7325\n",
            "Accuracy: 73.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmM1ftJe3yvK"
      },
      "source": [
        "## 3) Análisis final\n",
        "\n",
        "> Analiza con detalle el mejor clasificador. Busca un ejemplo mal clasificado de cada clase, justifica el error ¿se te ocurre alguna forma de solucionarlo?\n",
        "> \n",
        "> Compara los resultados obtenidos con y sin word embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU0UFbSm9NQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe5056f-c500-4ba7-80ff-c812ae378e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sin word embeddings pre-entrenados\n",
            "Accuracy: 72.38%\n",
            "Con word embeddings pre-entrenados congelados\n",
            "Accuracy: 69.73%\n",
            "Con word embeddings pre-entrenados sin congelar\n",
            "Accuracy: 73.25%\n"
          ]
        }
      ],
      "source": [
        "print(\"Sin word embeddings pre-entrenados\")\n",
        "print(\"Accuracy: %.2f%%\" % (score1[1]*100))\n",
        "print(\"Con word embeddings pre-entrenados congelados\")\n",
        "print(\"Accuracy: %.2f%%\" % (score2[1]*100))\n",
        "print(\"Con word embeddings pre-entrenados sin congelar\")\n",
        "print(\"Accuracy: %.2f%%\" % (score3[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El mejor resultado se ha conseguido usando el modelo con Word Embeddings preentrenados y permitiendo que se entrene con nuestros documentos.\n",
        "\n",
        "El peor resultado es con modelo preentrenado pero sin permitir entrenamiento. \n",
        "\n",
        "Esto puede deberse a que nuestro conjunto de entrenamiento ya es suficientemente grande (40K), por lo que no tenemos el problema de falta de datos para entrenar embedding desde 0. Por eso se ha obtenido un resultado mejor partiendo de un embedding desde 0."
      ],
      "metadata": {
        "id": "rfk2APRwpYF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = np.array([1 if probability > 0.5 else 0 for row in model1.predict(x_test) for probability in row])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzlnaLY361OA",
        "outputId": "a59b4e69-c6c1-4ea6-b981-9efd499713ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos que hay muchas muestras que no han sido clasificadas correctamente"
      ],
      "metadata": {
        "id": "GMC36udK8S3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(y_test_pred != y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmtlNHkS8V2b",
        "outputId": "1eb6c83a-7168-4948-fee6-d4527dc518d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2762"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[(np.array([tokenizer.index_word.get(idx) for idx in seq]),pred) \n",
        "    for pred, seq in zip(y_test_pred[y_test_pred != y_test.values],x_test[y_test_pred != y_test.values])][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgBJEniR6_Kl",
        "outputId": "beed67d5-b20f-4cd1-c698-0b25f6ec3ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array(['masterpiece', 'clearly', 'outstanding', 'well', 'pace', 'slow',\n",
              "         'cinematography', 'beautiful', 'next', 'time', 'tv', 'youll',\n",
              "         'never', 'see', 'better', 'tv', 'movie', 'never', 'felt', 'good'],\n",
              "        dtype='<U14'),\n",
              "  1),\n",
              " (array(['movie', 'would', 'really', 'recommend', 'see', 'apart',\n",
              "         'probably', 'going', 'like', 'especially', 'lot', 'talking',\n",
              "         'animals', 'ill', 'give', 'movie', 'rating', 'stars', 'possible',\n",
              "         'stars'], dtype='<U10'),\n",
              "  1),\n",
              " (array(['likely', 'enough', 'good', 'look', 'favorite', 'dr', 'hours',\n",
              "         'actually', 'number', 'like', 'evil', 'course', 'look', 'four',\n",
              "         'laugh', 'youll', 'love', 'scifi', 'ones', 'fantastic'],\n",
              "        dtype='<U9'),\n",
              "  1),\n",
              " (array(['later', 'place', 'made', 'things', 'may', 'help', 'movie', 'plot',\n",
              "         'could', 'made', 'interesting', 'question', 'mind', 'movie',\n",
              "         'ended', 'movie', 'really', 'ended', 'movie', 'ended'],\n",
              "        dtype='<U11'),\n",
              "  0),\n",
              " (array([None, None, None, None, None, 'earth', 'look', 'behind', 'scene',\n",
              "         'club', 'way', 'deal', 'various', 'problems', 'general', 'running',\n",
              "         'club', 'mention', 'car', 'well'], dtype=object),\n",
              "  0)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya hemos mostrado un par de ejemplos en el que nuestro clasificador ha fallado. Sin embargo, la secuencia de palabras no da mucha información sobre que decía el mensaje, por lo que procedemos a buscar un ejemplo.\n",
        "\n",
        "En este caso, buscamos una reseña con sentimiento negativo que use la palabra `\"masterpiece\"`, que probablemente concuerde con la predicción errónea que hemos hecho (pues no imagino que haya muchas reseñas negativas que usen palabras como `\"masterpiece\"`)"
      ],
      "metadata": {
        "id": "aBH5a_pveVSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for num, data in enumerate(zip(df.sentiment,df.reviews)):\n",
        "    sentiment, review = data\n",
        "    if 'masterpiece' in review and sentiment == 0:\n",
        "        print(num)\n",
        "        print(review)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH6HBrte7nX0",
        "outputId": "011442b8-8a95-453c-d18a-c65624c5133b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71\n",
            "honestly short film sucks dummy used necro scene pretty well made still phony enough looking ruin viewing experience unearthed dvd crisp clear havent made mind helps hinders film little grainy might added creepiness factor going idea film much hype surrounding subject matter honest necrophilia scenes films like nekromantik visitor q among others shocking aftermath talk film loneliness manner deep philosophy bull expensive beautifully filmed turd shocking disgusting insist viewing rent give fact many people make explicit movies necrophilia definitely bigger selection us sickos filming good gore watching rubbery looking doll get cut open considered gore absolutely nothing going overhyped mess hand genesis cerdas sequel aftermath available double feature released unearthed films absolute masterpiece short film really showing good director cerda really given right material although dont care aftermath genesis well made forgive cerda definitely keep eye future\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La reseña que encontramos parece concordar con la clasificaión errónea que hemos realizado, pues aunque es negativa sí que incluye todas palabras de la secuencia, que tienen connotaciones positivas."
      ],
      "metadata": {
        "id": "NAXStD6ufIqP"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}